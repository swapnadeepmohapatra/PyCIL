{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "We implemented `iCaRL+RMM`, `FOSTER+RMM` in [rmm.py](models/rmm.py).  We implemented the `Pretraining Stage` of `RMM` in [rmm_train.py](rmm_train.py). \n",
    "Use the following training script to run it.\n",
    "```bash\n",
    "python rmm_train.py --config=./exps/rmm-pretrain.json\n",
    "```\n",
    "'''\n",
    "import json\n",
    "import argparse\n",
    "from trainer import train\n",
    "import sys\n",
    "import logging\n",
    "import copy\n",
    "import torch\n",
    "from utils import factory\n",
    "from utils.data_manager import DataManager\n",
    "from utils.rl_utils.ddpg import DDPG\n",
    "from utils.rl_utils.rl_utils import ReplayBuffer\n",
    "from utils.toolkit import count_parameters\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class CILEnv:\n",
    "    def __init__(self, args) -> None:\n",
    "        self._args = copy.deepcopy(args)\n",
    "        self.settings = [(50, 2), (50, 5), (50, 10), (50, 20), (10, 10), (20, 20), (5, 5)]\n",
    "        self.settings = [(5,5)] #  Debug\n",
    "        self._args[\"init_cls\"], self._args[\"increment\"] = self.settings[np.random.randint(len(self.settings))]\n",
    "        self.data_manager = DataManager(\n",
    "            self._args[\"dataset\"],\n",
    "            self._args[\"shuffle\"],\n",
    "            self._args[\"seed\"],\n",
    "            self._args[\"init_cls\"],\n",
    "            self._args[\"increment\"],\n",
    "        )\n",
    "        self.model = factory.get_model(self._args[\"model_name\"], self._args)\n",
    "\n",
    "    @property\n",
    "    def nb_task(self):\n",
    "        return self.data_manager.nb_tasks\n",
    "\n",
    "    @property\n",
    "    def cur_task(self):\n",
    "        return self.model._cur_task\n",
    "\n",
    "    def get_task_size(self, task_id):\n",
    "        return self.data_manager.get_task_size(task_id)\n",
    "\n",
    "    def reset(self):\n",
    "        self._args[\"init_cls\"], self._args[\"increment\"] = self.settings[np.random.randint(len(self.settings))]\n",
    "        self.data_manager = DataManager(\n",
    "            self._args[\"dataset\"],\n",
    "            self._args[\"shuffle\"],\n",
    "            self._args[\"seed\"],\n",
    "            self._args[\"init_cls\"],\n",
    "            self._args[\"increment\"],\n",
    "        )\n",
    "        self.model = factory.get_model(self._args[\"model_name\"], self._args)\n",
    "\n",
    "        info = \"start new task:  dataset: {}, init_cls: {},  increment: {}\".format(\n",
    "            self._args[\"dataset\"], self._args[\"init_cls\"], self._args[\"increment\"]\n",
    "        )\n",
    "        return np.array([self.get_task_size(0) / 100, 0]), None, False, info\n",
    "\n",
    "    def step(self, action):\n",
    "        self.model._m_rate_list.append(action[0])\n",
    "        self.model._c_rate_list.append(action[1])\n",
    "        self.model.incremental_train(self.data_manager)\n",
    "        cnn_accy, nme_accy = self.model.eval_task()\n",
    "        self.model.after_task()\n",
    "        done = self.cur_task == self.nb_task - 1\n",
    "        info = \"running task [{}/{}]:  dataset: {}, increment: {}, cnn_accy top1: {},  top5: {}\".format(\n",
    "            self.model._known_classes,\n",
    "            100,\n",
    "            self._args[\"dataset\"],\n",
    "            self._args[\"increment\"],\n",
    "            cnn_accy[\"top1\"],\n",
    "            cnn_accy[\"top5\"],\n",
    "        )\n",
    "        return (\n",
    "            np.array(\n",
    "                [\n",
    "                    self.get_task_size(self.cur_task+1)/100 if not done else 0.,\n",
    "                    self.model.memory_size\n",
    "                    / (self.model.memory_size + self.model.new_memory_size),\n",
    "                ]\n",
    "            ),\n",
    "            cnn_accy[\"top1\"]/100,\n",
    "            done,\n",
    "            info,\n",
    "        )\n",
    "\n",
    "\n",
    "def _train(args):\n",
    "\n",
    "    logs_name = \"logs/RL-CIL/{}/\".format(args[\"model_name\"])\n",
    "    if not os.path.exists(logs_name):\n",
    "        os.makedirs(logs_name)\n",
    "\n",
    "    logfilename = \"logs/RL-CIL/{}/{}_{}_{}_{}_{}\".format(\n",
    "        args[\"model_name\"],\n",
    "        args[\"prefix\"],\n",
    "        args[\"seed\"],\n",
    "        args[\"model_name\"],\n",
    "        args[\"convnet_type\"],\n",
    "        args[\"dataset\"],\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(filename)s] => %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(filename=logfilename + \".log\"),\n",
    "            logging.StreamHandler(sys.stdout),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    _set_random()\n",
    "    _set_device(args)\n",
    "    print_args(args)\n",
    "\n",
    "    actor_lr = 5e-4\n",
    "    critic_lr = 5e-3\n",
    "    num_episodes = 200\n",
    "    hidden_dim = 32\n",
    "    gamma = 0.98\n",
    "    tau = 0.005\n",
    "    buffer_size = 1000\n",
    "    minimal_size = 50\n",
    "    batch_size = 32\n",
    "    sigma = 0.2 # action noise, encouraging the off-policy algo to explore.\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    env = CILEnv(args)\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    agent = DDPG(\n",
    "        2, 1, 4, hidden_dim, False, 1, sigma, actor_lr, critic_lr, tau, gamma, device\n",
    "    )\n",
    "    for iteration in range(num_episodes):\n",
    "        state, *_, info = env.reset()\n",
    "        logging.info(info)\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.take_action(state)\n",
    "            logging.info(f\"take action: m_rate {action[0]}, c_rate {action[1]}\")\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            logging.info(info)\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if replay_buffer.size() > minimal_size:\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    \"states\": b_s,\n",
    "                    \"actions\": b_a,\n",
    "                    \"next_states\": b_ns,\n",
    "                    \"rewards\": b_r,\n",
    "                    \"dones\": b_d,\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "\n",
    "def _set_device(args):\n",
    "    device_type = args[\"device\"]\n",
    "    gpus = []\n",
    "\n",
    "    for device in device_type:\n",
    "        if device_type == -1:\n",
    "            device = torch.device(\"cpu\")\n",
    "        else:\n",
    "            device = torch.device(\"cuda:{}\".format(device))\n",
    "\n",
    "        gpus.append(device)\n",
    "\n",
    "    args[\"device\"] = gpus\n",
    "\n",
    "\n",
    "def _set_random():\n",
    "    random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "    torch.cuda.manual_seed(1)\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def print_args(args):\n",
    "    for key, value in args.items():\n",
    "        logging.info(\"{}: {}\".format(key, value))\n",
    "\n",
    "\n",
    "def train(args):\n",
    "    seed_list = copy.deepcopy(args[\"seed\"])\n",
    "    device = copy.deepcopy(args[\"device\"])\n",
    "\n",
    "    for seed in seed_list:\n",
    "        args[\"seed\"] = seed\n",
    "        args[\"device\"] = device\n",
    "        _train(args)\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = setup_parser().parse_args()\n",
    "    param = load_json(args.config)\n",
    "    args = vars(args)  # Converting argparse Namespace to a dict.\n",
    "    args.update(param)  # Add parameters from json\n",
    "\n",
    "    train(args)\n",
    "\n",
    "\n",
    "def load_json(settings_path):\n",
    "    with open(settings_path) as data_file:\n",
    "        param = json.load(data_file)\n",
    "\n",
    "    return param\n",
    "\n",
    "\n",
    "def setup_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Reproduce of multiple continual learning algorthms.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--config\",\n",
    "        type=str,\n",
    "        default=\"./exps/finetune.json\",\n",
    "        help=\"Json file of settings.\",\n",
    "    )\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
